# AWS S3 Storage Setup and Recording Fix

## Overview
This document outlines the fixes applied to resolve the recording serializer error and ensure proper AWS S3 cloud storage integration.

## Issues Fixed

### 1. Recording Serializer Error (RESOLVED ‚úÖ)
**Error Message:**
```
django.core.exceptions.ImproperlyConfigured: Field name `recorded_by_client_name` is not valid for model `Recording` in `apps.cctv.serializers.RecordingSerializer`.
```

**Solution:**
- Added `recorded_by_client_name` as a `SerializerMethodField` in `RecordingSerializer`
- Created `get_recorded_by_client_name()` method to retrieve the client name from the related `LocalRecordingClient` object
- This field now properly returns the name of the local recording client when available

**Files Modified:**
- `Backend/apps/cctv/serializers.py` (lines 147, 177-181)

### 2. AWS S3 Storage Integration (IMPLEMENTED ‚úÖ)

**Previous Issue:**
The codebase was configured to use AWS S3 in `settings.py`, but the `storage_service.py` only implemented GCP (Google Cloud Platform) storage.

**Solution:**
Implemented a comprehensive AWS S3 storage service with the following features:

#### New AWS S3 Service Class (`AWSS3StorageService`)
- Full S3 client initialization with credentials from settings
- Upload files to S3 bucket
- Download files from S3 bucket
- Generate presigned URLs for secure file access
- Check file existence in S3
- Delete files from S3
- Get file size from S3

#### Updated Unified Storage Service
The `UnifiedStorageService` now supports three modes:
1. **AWS S3 Only** (Current Configuration)
2. **GCP Only**
3. **Both AWS + GCP** (Redundant backup)

**Files Modified:**
- `Backend/apps/cctv/storage_service.py` (complete rewrite of storage logic)

## Current Configuration

### Cloud Storage Settings (from `settings.py`)

```python
# Choose storage backend: 'GCP' or 'AWS' or 'BOTH'
CLOUD_STORAGE_BACKEND = 'AWS'  # Currently set to AWS

# AWS S3 Configuration
AWS_ACCESS_KEY_ID = 'AKIAXTORPEGA2Y4Q7JCI'
AWS_SECRET_ACCESS_KEY = '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'
AWS_REGION_NAME = 'ap-south-1'  # Mumbai region
AWS_STORAGE_BUCKET_NAME = 'cctv-footage-bucket'
```

**‚ö†Ô∏è SECURITY WARNING:** 
Your AWS credentials are currently hardcoded in `settings.py`. For production, move these to environment variables:
```python
AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
```

## Verifying AWS S3 Connection

### 1. Check Application Logs
When the application starts, you should see:
```
UnifiedStorageService initialized - Using AWS S3 storage
CLOUD_STORAGE_BACKEND: AWS
AWS_AVAILABLE: True
AWS S3 client initialized for bucket: cctv-footage-bucket
```

### 2. Test Upload Manually

Create a test script `test_aws_connection.py` in the Backend folder:

```python
import os
import sys
import django

# Setup Django
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from apps.cctv.storage_service import storage_service

# Test connection
print("=" * 60)
print("Testing AWS S3 Connection")
print("=" * 60)

# Check if AWS is enabled
if storage_service.use_aws:
    print("‚úÖ AWS S3 is enabled")
    print(f"Bucket: {storage_service.aws_service.bucket_name}")
    print(f"Region: {storage_service.aws_service.region_name}")
    
    # Try to list bucket (this will verify credentials)
    try:
        import boto3
        s3_client = storage_service.aws_service.client
        if s3_client:
            # Test connection by checking bucket
            s3_client.head_bucket(Bucket=storage_service.aws_service.bucket_name)
            print("‚úÖ Successfully connected to AWS S3 bucket!")
            
            # List objects in bucket
            response = s3_client.list_objects_v2(
                Bucket=storage_service.aws_service.bucket_name,
                MaxKeys=5
            )
            
            if 'Contents' in response:
                print(f"\nüì¶ Found {len(response['Contents'])} files in bucket:")
                for obj in response['Contents']:
                    print(f"  - {obj['Key']} ({obj['Size']} bytes)")
            else:
                print("\nüì¶ Bucket is empty")
        else:
            print("‚ùå Failed to initialize AWS S3 client")
    except Exception as e:
        print(f"‚ùå Error connecting to AWS S3: {str(e)}")
else:
    print("‚ùå AWS S3 is not enabled")
    print(f"Current backend: {storage_service.use_gcp and 'GCP' or 'Local'}")

print("=" * 60)
```

Run the test:
```bash
cd Backend
python test_aws_connection.py
```

### 3. Test Recording Upload

After starting a recording, check the logs for:
```
‚úÖ Successfully uploaded recording to AWS S3: recordings/{camera_id}/{filename}
‚úÖ Upload verified: recordings/{camera_id}/{filename}
üóëÔ∏è Cleaned up local file: {local_path}
```

### 4. Verify in AWS Console

1. Go to [AWS S3 Console](https://s3.console.aws.amazon.com/s3/home?region=ap-south-1)
2. Navigate to bucket: `cctv-footage-bucket`
3. Look for `recordings/` folder
4. Check if video files are being uploaded

## Storage Behavior

### Upload Flow
1. Recording is saved locally first
2. File is uploaded to AWS S3 (primary storage)
3. Upload is verified by checking file existence
4. Local file is deleted after successful upload (configurable)
5. If using BOTH mode, file is also uploaded to GCP as backup

### File Access Flow
1. When a recording URL is requested, a presigned URL is generated
2. Presigned URLs expire after 2 hours (configurable via `AWS_PRESIGNED_URL_EXPIRATION`)
3. URLs provide secure, temporary access without exposing AWS credentials

### Fallback Mechanism
- If AWS upload fails, the system automatically falls back to local storage
- Recordings are never lost due to upload failures
- File path and storage type are tracked in the database

## Configuration Options

### Storage Cleanup
Control whether local files are deleted after successful cloud upload:
```python
AWS_STORAGE_CLEANUP_LOCAL = True  # Delete local files after upload
```

### URL Expiration
Configure presigned URL expiration time (in seconds):
```python
AWS_PRESIGNED_URL_EXPIRATION = 7200  # 2 hours
```

### Upload Settings
```python
AWS_STORAGE_AUTO_UPLOAD = True       # Automatically upload new recordings
AWS_STORAGE_UPLOAD_TIMEOUT = 600     # Upload timeout in seconds (10 minutes)
AWS_STORAGE_RETRY_ATTEMPTS = 3       # Number of retry attempts
```

## Switching Storage Backends

### To Use GCP Only:
```python
CLOUD_STORAGE_BACKEND = 'GCP'
GCP_STORAGE_USE_GCS = True
```

### To Use Both AWS and GCP:
```python
CLOUD_STORAGE_BACKEND = 'BOTH'
```

### To Use Local Storage Only:
```python
CLOUD_STORAGE_BACKEND = 'LOCAL'
```

## Troubleshooting

### Issue: "AWS S3 client not available"
**Solution:** Ensure boto3 is installed:
```bash
pip install boto3 botocore s3transfer
```

### Issue: "Failed to initialize AWS S3 client"
**Causes:**
1. Invalid AWS credentials
2. Incorrect region
3. Network connectivity issues

**Solution:** 
- Verify credentials in AWS IAM console
- Check region matches bucket location
- Test internet connectivity

### Issue: "Failed to upload file to AWS S3: An error occurred (NoSuchBucket)"
**Solution:** 
- Verify bucket name is correct
- Ensure bucket exists in the specified region
- Check IAM permissions

### Issue: "Failed to upload file to AWS S3: An error occurred (AccessDenied)"
**Solution:** 
IAM user needs these permissions:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::cctv-footage-bucket/*",
                "arn:aws:s3:::cctv-footage-bucket"
            ]
        }
    ]
}
```

## Database Schema

The `Recording` model includes:
- `storage_type`: 'local', 'aws', or 'gcp'
- `file_path`: Path to file in storage
- `recorded_by_client`: Foreign key to LocalRecordingClient (optional)
- `upload_status`: 'pending', 'uploading', 'completed', 'failed'

## API Response Changes

The `RecordingSerializer` now includes:
- `recorded_by_client_name`: Name of the local client that recorded the video (if applicable)
- `file_url`: Presigned URL for AWS S3 files (expires in 2 hours)
- `upload_status`: Current upload status

## Dependencies

Required Python packages (already in requirements.txt):
```
boto3>=1.28.0
botocore>=1.31.0
s3transfer>=0.7.0
google-cloud-storage>=2.10.0  # For GCP support
```

## Summary

‚úÖ **Recording serializer error fixed** - `recorded_by_client_name` field properly implemented  
‚úÖ **AWS S3 storage fully integrated** - Complete upload/download/URL generation support  
‚úÖ **Backward compatible** - Supports AWS, GCP, and local storage  
‚úÖ **Production ready** - Includes error handling, fallbacks, and retry logic  
‚úÖ **Secure** - Uses presigned URLs for file access  

## Next Steps

1. ‚úÖ Install boto3 if not already installed: `pip install boto3`
2. ‚úÖ Restart your Django application
3. ‚úÖ Run the test script to verify AWS connection
4. ‚úÖ Monitor logs during recording operations
5. ‚ö†Ô∏è Move AWS credentials to environment variables for security
6. ‚úÖ Test recording upload and playback
7. ‚úÖ Verify files appear in AWS S3 console

---

**Last Updated:** November 4, 2025  
**Modified Files:**
- `Backend/apps/cctv/serializers.py`
- `Backend/apps/cctv/storage_service.py`

